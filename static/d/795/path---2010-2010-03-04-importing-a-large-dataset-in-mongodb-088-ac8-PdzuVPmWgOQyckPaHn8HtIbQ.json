{"data":{"markdownRemark":{"html":"<p>Last night I imported a large, but not outrageously large, dataset into a MongoDB database. The complication might have been the structure of each document. They weren't too complicated, just a hash with two keys, one having a string as a value, and the other having an array.</p>\n<p>At first I was using Ruby1.9.1, but it was taking too long so I switched to using the command line interface, \"mongoimport\".</p>\n<p>The cool thing about this is that it can import JSON directly. I converted my data set to JSON format, saved it to a single file with about 23,000 JSON objects, and then ran it:</p>\n<pre class=\"sh_sh\">mongoimport --host 192.168.8.103 --db doculabsappone -c tags < tmp/tags.json</pre>\n<p>It was <strong>way</strong> faster than using Ruby1.9.1!</p>\n<p>Â¥</p>","frontmatter":{"title":"Importing a Large Dataset in MongoDB","date":"March 4th, 2010"}}},"pageContext":{"isCreatedByStatefulCreatePages":false,"slug":"/2010/2010-03-04-importing-a-large-dataset-in-mongodb/"}}