{
    "componentChunkName": "component---src-templates-blog-post-js",
    "path": "/2010/03/importing-a-large-dataset-in-mongodb/",
    "result": {"data":{"site":{"siteMetadata":{"domain":"https://www.docunext.com"}},"markdownRemark":{"html":"<p>Last night I imported a large, but not outrageously large, dataset into a MongoDB database. The complication might have been the structure of each document. They weren't too complicated, just a hash with two keys, one having a string as a value, and the other having an array.</p>\n<p>At first I was using Ruby1.9.1, but it was taking too long so I switched to using the command line interface, \"mongoimport\".</p>\n<p>The cool thing about this is that it can import JSON directly. I converted my data set to JSON format, saved it to a single file with about 23,000 JSON objects, and then ran it:</p>\n<pre class=\"sh_sh\">mongoimport --host 192.168.8.103 --db doculabsappone -c tags < tmp/tags.json</pre>\n<p>It was <strong>way</strong> faster than using Ruby1.9.1!</p>\n<p>Â¥</p>","fields":{"slug":"/2010/03/importing-a-large-dataset-in-mongodb/"},"frontmatter":{"title":"Importing a Large Dataset in MongoDB","date":"March 4th, 2010","tags":"json,mongodb,ruby"}}},"pageContext":{"slug":"/2010/03/importing-a-large-dataset-in-mongodb/"}},
    "staticQueryHashes": ["2191495970","4224293195"]}