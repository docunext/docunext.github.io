{"componentChunkName":"component---src-templates-blog-post-js","path":"/2007/01/duplicity-to-dreamhost-and-amazon-s3/","result":{"data":{"markdownRemark":{"html":"<p>Howto use duplicity to back to dreamhost.com accounts and Amazon S3:</p>\n<h3>Dreamhost</h3>\n<p>Dreamhost has some amazing offerings, one of which is a huge amount of storage, that increases over time! Using duplicity there is easy because you can simply ssh, or even better: scp, into it.</p>\n<p>From the manual page of duplicity (i.e. \"man duplicity\") :</p>\n<pre>duplicity --full /home/me scp://uid@other.host/some_dir</pre>\n<p>Put your username in where it has \"uid\", making sure you have shell access at dreamhost for that user. Put your domain name which you host at dreamhost, such as example.com. Make sure whatever directory you choose exists in your home directory at dreamhost. I chose duplicity_backups so it would be easy to recognize.</p>\n<p>Duplicity will then ask you for a gpg passphrase. Make it a good one. Since the first full backup will take awhile, I started the session by typing \"screen\", that way I could exit the shell with CNTL-a-d, and go on to other things. To return to that terminal session, just type \"screen -r\".</p>\n<p>Strange thing though is the low throughput rates. On a tier-1 backbone connection, I'm only getting top speeds of 50KB/s, which is only about 400kbps. What gives? I should be at least getting two to three times that speed. Oh well, perhaps dreamhost.com does some ingress throttling to dampen any adverse network latency effects.</p>\n<p><strong>Word to the wise:</strong> Start small, if during your initial \"first\" backup, the transfer fails, you will have to start all over. To me that it is rediculous, but I guess its the cost of encryption.</p>\n<h3>Amazon S3</h3>\n<p>This isn't so easy. I'm trying duplicity with BitBucket, with directions from here:</p>\n<p><a href=\"http://labora.harnvi.net/?p=34\">http://labora.harnvi.net/?p=34</a></p>\n<p>But I'm getting these errors when I try to run duplicity-bin.</p>\n<pre>ImportError: No module named _librsync</pre>\n<p>Ugh that's just not going to work.</p>\n<p>OK, found this in CVS-Readme:</p>\n<pre>CVS README - Notes for people checking out of CVS-------------------------------------------------\n\nGetting duplicity to run:-------------------------\n\nIf you want to run a version of duplicity checked out of CVS into your$DUP_ROOT directory, change to $DUP_ROOT/duplicity and run the./compilec.py file.  With any luck, a _librsync.so library will appear\n\nin that directory.  Then run duplicity-bin, making sure that all the\n\nfiles are in your PYTHONPATH:\n\nPYTHONPATH=$DUP_ROOT duplicity-bin\n\nor\n\nPYTHONPATH=$DUP_ROOT rdiffdir\n\nRunning the unit tests:-----------------------\n\nIf you want to try some of tests, you first have to untar the\n\ntestfiles.tar.gz as root (the tarball contains device files, files\n\nwith various uid/gid, etc):\n\ncd testing; tar -xvzf testfiles.tar.gz\n\nThen run the various *test.py files, for instance:\n\ncd testing; python lazytest.py</pre>\n<p>Except now I'm getting: error: command 'gcc' failed with exit status 1</p>\n<p>Argh. OK so I couldn't get <em>librsync.so to compile, but I installed rdiff</em>backup with:</p>\n<pre>apt-get install rdiff_backup</pre>\n<p>and then copied the _librsync.so to the duplicity src directory.</p>\n<pre>cp /var/lib/python-support/python2.4/rdiff_backup/_librsync.so duplicity</pre>\n<p>Then got the bitbucket:</p>\n<pre>wget http://cheeseshop.python.org/packages/source/B/BitBucket/BitBucket-0.4a.tar.gz\n\ntar -xzvf BitBucket-0.4a.tar.gz\n\ncd BitBucket-0.4a\n\npython setup.py install</pre>\n<p>Holy cow - it works! Gosh I do wish I could get it to show me how quickly the files are loading.</p>\n<p>Â¥</p>","frontmatter":{"title":"Duplicity to dreamhost and Amazon S3","date":"January 28th, 2007"}}},"pageContext":{"slug":"/2007/01/duplicity-to-dreamhost-and-amazon-s3/"}}}