<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta http-equiv="x-ua-compatible" content="ie=edge"/><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"/><style data-href="/styles.742409eb857ab2ccca34.css" data-identity="gatsby-global-css">li{margin-right:1rem}h1,li{display:inline-block}h1{font-style:normal;margin-top:0}header{margin-bottom:1.5rem}header ul{float:right;list-style:none}#wrapper{margin:3em auto;max-width:700px;padding:10px}.blogPostDate{color:#333;font-size:8pt;margin-bottom:9px;padding-top:4px}</style><meta name="generator" content="Gatsby 4.4.0"/><title data-react-helmet="true">mdadm raid kernel panic resync</title><link data-react-helmet="true" rel="canonical" href="https://www.docunext.com/2007/06/mdadm/"/><link as="script" rel="preload" href="/webpack-runtime-910253975eed29c257e8.js"/><link as="script" rel="preload" href="/framework-e27ef662c918f795a5a3.js"/><link as="script" rel="preload" href="/app-578f356f1ad3d3b2b65e.js"/><link as="script" rel="preload" href="/commons-bbb2797d08d51fd1bd26.js"/><link as="script" rel="preload" href="/component---src-templates-blog-post-js-cc8ebf9459c2ce2cf980.js"/><link as="fetch" rel="preload" href="/page-data/2007/06/mdadm/page-data.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/sq/d/2191495970.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/sq/d/4224293195.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/app-data.json" crossorigin="anonymous"/></head><body><div id="___gatsby"><div style="outline:none" tabindex="-1" id="gatsby-focus-wrapper"><div id="wrapper"><header><ul><li><a href="/about/">About</a></li></ul><a href="/"><h1>Docunext</h1></a></header><hr/><div><h2>mdadm raid kernel panic resync</h2><div class="blogPostDate">June 3rd, 2007</div><div><p>For whatever reason, my file server crash last night. I'm now trying to fix it. I booted into single user mode (had to cancel the fsck that was trying to happen at the same time as a raid sync, which made for slow progress).</p>
<p>Here's the output of "cat /proc/mdstat":</p>
<p>Seems to moving right along, its now at 24.7%. Good!</p>
<p>UPDATE: BAD!</p>
<p>Turns out the machine is throwing a kernel panic at about 47.3% through the raid resync. Not only is it a kernel panic, it is a machine check error = hardware failure. While what I've read so far says that MCEs are usually caused by chips, I think this one is caused by a hard drive.</p>
<p><a href="http://kerneltrap.org/node/4993"><a href="http://kerneltrap.org/node/4993">http://kerneltrap.org/node/4993</a></a></p>
<p>When I upgraded last night, I noticed that a new initrd was installed. Why? I don't know, but I booted with the original kernel and now the resync is almost at 50%, which could mean the MCE was caused by the new initrd. A possible cause? I've been running the system using "unstable" for a long time, and then switched to "testing". On the other hand, I am also copying a lot of files from the file server, causing the resync to slow to its limit of 1000, maybe that is allowing it to go further?</p>
<p>This screenshot says:</p>
<p>HARDWARE ERROR</p>
<p>CPU 0: Machine Check Exception 4 Bank 4: b2000000000000070f0f</p>
<p>TSC 1c3469a27ae</p>
<p>This is not a software problem!</p>
<p>Run through mcelog --ascii to decode and contact your hardware manufacturer</p>
<p>Kernel panic - not syncing: machine check.</p>
<p>Sorry, I'm not going to type out what that error says. Maybe Google will get OCR working one of these days.</p>
<p>UPDATE: Though the resync did not go all the way through, it threw a whole bunch of errors, but then started over again. :-) At least I can continue trying to remove what I had on there. This time the resync seems to be getting a little further... we're up to 49.4%!</p>
<p>HOORAY:</p>
<pre>Personalities : [raid0] [raid1] [raid5]
md0 : active raid1 sda1[0] sdd1[3] sdc1[2] sdb1[1]      32000 blocks [4/4] [UUUU]
md2 : active raid1 sdc2[0] sdd2[1]      9767424 blocks [2/2] [UU]
md4 : active raid5 sda6[4](F) sdd6[3] sdc6[2] sdb6[1]      348666432 blocks level 5, 64k chunk, algorithm 2 [4/3] [_UUU]
md5 : active raid5 sda7[0] sdd7[3] sdc7[2] sdb7[1]      348666432 blocks level 5, 64k chunk, algorithm 2 [4/4] [UUUU]
md3 : active raid1 sda5[0] sdd5[3] sdc5[2] sdb5[1]      1951744 blocks [4/4] [UUUU]
md1 : active raid1 sda2[0] sdb2[1]      9767424 blocks [2/2] [UU]
unused devices: &lt;none></pre>
<p>Now I've rebooted and am letting fsck do its thing.</p>
<p>After that, I had the same problems, but fsck went all the way through. I noticed that the initramfs-tools was still having trouble with the new initramfs.img, and low and behold the /boot/ partition was full. UGH! I fixed that, and now the newer kernel is working correctly again. However, my raid array is still degraded. Hmmm.</p>
<pre>cat /proc/mdstat
Personalities : [raid1] [raid6] [raid5] [raid4]
md0 : active raid1 sda1[0] sdd1[3] sdc1[2] sdb1[1]      32000 blocks [4/4] [UUUU]
md2 : active raid1 sdc2[0] sdd2[1]      9767424 blocks [2/2] [UU]
md3 : active raid1 sda5[0] sdd5[3] sdc5[2] sdb5[1]      1951744 blocks [4/4] [UUUU]
md4 : active raid5 sdb6[1] sdd6[3] sdc6[2]      348666432 blocks level 5, 64k chunk, algorithm 2 [4/3] [_UUU]
md5 : active raid5 sda7[0] sdd7[3] sdc7[2] sdb7[1]      348666432 blocks level 5, 64k chunk, algorithm 2 [4/4] [UUUU]
md1 : active raid1 sda2[0] sdb2[1]      9767424 blocks [2/2] [UU]
unused devices: &lt;none></pre>
<p>I added the missing partition:</p>
<pre>mdadm /dev/md4 -a /dev/sda6</pre>
<p>then the raid started rebuilding again:</p>
<pre>cat /proc/mdstat
Personalities : [raid1] [raid6] [raid5] [raid4]
md0 : active raid1 sda1[0] sdd1[3] sdc1[2] sdb1[1]      32000 blocks [4/4] [UUUU]
md2 : active raid1 sdc2[0] sdd2[1]      9767424 blocks [2/2] [UU]
md3 : active raid1 sda5[0] sdd5[3] sdc5[2] sdb5[1]      1951744 blocks [4/4] [UUUU]
md4 : active raid5 sda6[4] sdb6[1] sdd6[3] sdc6[2]      348666432 blocks level 5, 64k chunk, algorithm 2 [4/3] [_UUU]      [>....................]  recovery =  0.1% (191348/116222144) finish=50.5min speed=38269K/sec
md5 : active raid5 sda7[0] sdd7[3] sdc7[2] sdb7[1]      348666432 blocks level 5, 64k chunk, algorithm 2 [4/4] [UUUU]
md1 : active raid1 sda2[0] sdb2[1]      9767424 blocks [2/2] [UU]
unused devices: &lt;none></pre>
<h4>CONCLUSION?</h4>
<p>The fact that this went down the way it did screams the fact that redundancy rules, software based RAID rules, and mdadm rules.  'Nuff said.</p></div></div><div><span>Yearly Indexes: </span><span><a href="/2003/"><span>2003</span></a> </span><span><a href="/2004/"><span>2004</span></a> </span><span><a href="/2006/"><span>2006</span></a> </span><span><a href="/2007/"><span>2007</span></a> </span><span><a href="/2008/"><span>2008</span></a> </span><span><a href="/2009/"><span>2009</span></a> </span><span><a href="/2010/"><span>2010</span></a> </span><span><a href="/2011/"><span>2011</span></a> </span><span><a href="/2012/"><span>2012</span></a> </span><span><a href="/2013/"><span>2013</span></a> </span><span><a href="/2015/"><span>2015</span></a> </span><span><a href="/2019/"><span>2019</span></a> </span><span><a href="/2020/"><span>2020</span></a> </span><span><a href="/2022/"><span>2022</span></a> </span></div></div></div><div id="gatsby-announcer" style="position:absolute;top:0;width:1px;height:1px;padding:0;overflow:hidden;clip:rect(0, 0, 0, 0);white-space:nowrap;border:0" aria-live="assertive" aria-atomic="true"></div></div><script id="gatsby-script-loader">/*<![CDATA[*/window.pagePath="/2007/06/mdadm/";/*]]>*/</script><script id="gatsby-chunk-mapping">/*<![CDATA[*/window.___chunkMapping={"polyfill":["/polyfill-9068a959998310636008.js"],"app":["/app-578f356f1ad3d3b2b65e.js"],"component---src-pages-about-js":["/component---src-pages-about-js-738961a9611f9aa4e8d4.js"],"component---src-pages-games-react-nuzaq-js":["/component---src-pages-games-react-nuzaq-js-50d5da494a82a1aaeb8b.js"],"component---src-pages-index-js":["/component---src-pages-index-js-3aa2421409c5653df938.js"],"component---src-pages-years-js":["/component---src-pages-years-js-d36f319c6c16ef9e8314.js"],"component---src-templates-blog-post-js":["/component---src-templates-blog-post-js-cc8ebf9459c2ce2cf980.js"],"component---src-templates-year-js":["/component---src-templates-year-js-e8af3ddab603f61c6142.js"]};/*]]>*/</script><script src="/polyfill-9068a959998310636008.js" nomodule=""></script><script src="/component---src-templates-blog-post-js-cc8ebf9459c2ce2cf980.js" async=""></script><script src="/commons-bbb2797d08d51fd1bd26.js" async=""></script><script src="/app-578f356f1ad3d3b2b65e.js" async=""></script><script src="/framework-e27ef662c918f795a5a3.js" async=""></script><script src="/webpack-runtime-910253975eed29c257e8.js" async=""></script></body></html>